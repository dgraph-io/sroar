// Code generated by command: go run asm.go -out asm_x86.s -stubs asm_x86.go. DO NOT EDIT.

#include "textflag.h"

// func asmBitmapOr(data []uint16, other []uint16) int
// Requires: AVX, AVX2, POPCNT
TEXT ·asmBitmapOr(SB), NOSPLIT|NOPTR, $0-56
	MOVQ data_base+0(FP), AX
	MOVQ other_base+24(FP), CX
	VZEROALL
	XORQ DX, DX
	XORQ BX, BX
	XORQ SI, SI
	XORQ DI, DI
	XORQ R8, R8
	XORQ R9, R9
	XORQ R10, R10
	XORQ R11, R11
	XORQ DX, DX
	XORQ DX, DX
	XORQ DX, DX
	XORQ DX, DX
	XORQ DX, DX
	XORQ DX, DX
	XORQ DX, DX
	XORQ DX, DX
	XORL R13, R13
	XORQ R12, R12

r:
	CMPL    R13, $0x00001000
	JE      done
	VMOVDQU (AX)(R13*2), Y0
	VMOVDQU 32(AX)(R13*2), Y1
	VPOR    (CX)(R13*2), Y0, Y0
	VPOR    32(CX)(R13*2), Y1, Y1
	VMOVDQU Y0, (AX)(R13*2)
	VMOVDQU Y1, 32(AX)(R13*2)
	POPCNTQ (AX)(R13*2), DX
	POPCNTQ 8(AX)(R13*2), BX
	POPCNTQ 16(AX)(R13*2), SI
	POPCNTQ 24(AX)(R13*2), DI
	POPCNTQ 32(AX)(R13*2), R8
	POPCNTQ 40(AX)(R13*2), R9
	POPCNTQ 48(AX)(R13*2), R10
	POPCNTQ 56(AX)(R13*2), R11
	ADDQ    DX, R12
	ADDQ    BX, R12
	ADDQ    SI, R12
	ADDQ    DI, R12
	ADDQ    R8, R12
	ADDQ    R9, R12
	ADDQ    R10, R12
	ADDQ    R11, R12
	ADDL    $0x00000020, R13
	JMP     r

done:
	MOVQ R12, ret+48(FP)
	RET

// func asmBitmapAnd(data []uint16, other []uint16, buf []uint16) int
// Requires: AVX, AVX2, POPCNT
TEXT ·asmBitmapAnd(SB), NOSPLIT|NOPTR, $0-80
	MOVQ data_base+0(FP), AX
	MOVQ other_base+24(FP), CX
	MOVQ buf_base+48(FP), DX
	VZEROALL
	XORQ BX, BX
	XORQ SI, SI
	XORQ DI, DI
	XORQ R8, R8
	XORQ R9, R9
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ BX, BX
	XORQ BX, BX
	XORQ BX, BX
	XORQ BX, BX
	XORQ BX, BX
	XORQ BX, BX
	XORQ BX, BX
	XORQ BX, BX
	XORL R14, R14
	XORQ R13, R13

r:
	CMPL    R14, $0x00001000
	JE      done
	VMOVDQU (AX)(R14*2), Y0
	VMOVDQU 32(AX)(R14*2), Y1
	VPAND   (CX)(R14*2), Y0, Y0
	VPAND   32(CX)(R14*2), Y1, Y1
	VMOVDQU Y0, (DX)(R14*2)
	VMOVDQU Y1, 32(DX)(R14*2)
	POPCNTQ (DX)(R14*2), BX
	POPCNTQ 8(DX)(R14*2), SI
	POPCNTQ 16(DX)(R14*2), DI
	POPCNTQ 24(DX)(R14*2), R8
	POPCNTQ 32(DX)(R14*2), R9
	POPCNTQ 40(DX)(R14*2), R10
	POPCNTQ 48(DX)(R14*2), R11
	POPCNTQ 56(DX)(R14*2), R12
	ADDQ    BX, R13
	ADDQ    SI, R13
	ADDQ    DI, R13
	ADDQ    R8, R13
	ADDQ    R9, R13
	ADDQ    R10, R13
	ADDQ    R11, R13
	ADDQ    R12, R13
	ADDL    $0x00000020, R14
	JMP     r

done:
	MOVQ R13, ret+72(FP)
	RET

// func asmBitmapAndNot(data []uint16, other []uint16, buf []uint16) int
// Requires: AVX, AVX2, POPCNT
TEXT ·asmBitmapAndNot(SB), NOSPLIT|NOPTR, $0-80
	MOVQ data_base+0(FP), AX
	MOVQ other_base+24(FP), CX
	MOVQ buf_base+48(FP), DX
	VZEROALL
	XORQ BX, BX
	XORQ SI, SI
	XORQ DI, DI
	XORQ R8, R8
	XORQ R9, R9
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ BX, BX
	XORQ BX, BX
	XORQ BX, BX
	XORQ BX, BX
	XORQ BX, BX
	XORQ BX, BX
	XORQ BX, BX
	XORQ BX, BX
	XORL R14, R14
	XORQ R13, R13

r:
	CMPL    R14, $0x00001000
	JE      done
	VMOVDQU (CX)(R14*2), Y0
	VMOVDQU 32(CX)(R14*2), Y1
	VPANDN  (AX)(R14*2), Y0, Y0
	VPANDN  32(AX)(R14*2), Y1, Y1
	VMOVDQU Y0, (DX)(R14*2)
	VMOVDQU Y1, 32(DX)(R14*2)
	POPCNTQ (DX)(R14*2), BX
	POPCNTQ 8(DX)(R14*2), SI
	POPCNTQ 16(DX)(R14*2), DI
	POPCNTQ 24(DX)(R14*2), R8
	POPCNTQ 32(DX)(R14*2), R9
	POPCNTQ 40(DX)(R14*2), R10
	POPCNTQ 48(DX)(R14*2), R11
	POPCNTQ 56(DX)(R14*2), R12
	ADDQ    BX, R13
	ADDQ    SI, R13
	ADDQ    DI, R13
	ADDQ    R8, R13
	ADDQ    R9, R13
	ADDQ    R10, R13
	ADDQ    R11, R13
	ADDQ    R12, R13
	ADDL    $0x00000020, R14
	JMP     r

done:
	MOVQ R13, ret+72(FP)
	RET
